{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Wikipedia articles for search\n",
    "\n",
    "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:39:41.692003Z",
     "start_time": "2024-07-24T16:39:41.263770Z"
    }
   },
   "source": [
    "# imports\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import os  # for environment variables\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "import dotenv  # for loading environment variables\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect documents\n",
    "\n",
    "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:40:56.022143Z",
     "start_time": "2024-07-24T16:40:51.109789Z"
    }
   },
   "source": [
    "# get Wikipedia pages about the 2022 Winter Olympics\n",
    "\n",
    "CATEGORY_TITLE = \"Category:2022 Winter Olympics\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category, max_depth: int\n",
    ") -> set[str]:\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "# ^note: max_depth=1 means we go one level deep in the category tree\n",
    "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 731 article titles in Category:2022 Winter Olympics.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:52:13.846466Z",
     "start_time": "2024-07-24T16:52:13.843682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# leave only first 10 titles for this example\n",
    "titles = list(titles)[:10]\n",
    "print(titles)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brad Gushue', 'Wolfgang Kindl', 'Nadia Delago', 'List of athletes not attending the 2022 Winter Olympics due to COVID-19 concerns', 'Francesco Friedrich', 'Alexander Nikishin', 'South Korea at the 2022 Winter Olympics', 'Morocco at the 2022 Winter Olympics', 'Candy Bauer', 'Ski jumping at the 2022 Winter Olympics â€“ Qualification']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Discard less relevant-looking sections like External Links and Footnotes\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:52:22.164957Z",
     "start_time": "2024-07-24T16:52:22.158009Z"
    }
   },
   "source": [
    "# define functions to split Wikipedia pages into sections\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,\n",
    "    parent_titles: list[str],\n",
    "    sections_to_ignore: set[str],\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results\n",
    "\n",
    "\n",
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:52:35.684428Z",
     "start_time": "2024-07-24T16:52:24.462102Z"
    }
   },
   "source": [
    "# split pages into sections\n",
    "# may take ~1 minute per 100 articles\n",
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 sections in 10 pages.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:54:01.196029Z",
     "start_time": "2024-07-24T16:54:01.192291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "pprint(wikipedia_sections[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Brad Gushue'],\n",
      " '{{Short description|Canadian curler (born 1980)}}\\n'\n",
      " '{{Infobox curler\\n'\n",
      " '| name = Brad Gushue\\n'\n",
      " '| image = Brad Gushue 2018 Elite 10-002.jpg\\n'\n",
      " '| caption = Brad Gushue watches his shot at the [[2018 Elite 10 (March)|2018 '\n",
      " 'Elite 10]] in Winnipeg, Manitoba.\\n'\n",
      " '| image_size = \\n'\n",
      " '| birth_date = {{birth date and age|1980|6|16}}\\n'\n",
      " \"| birth_place = [[St. John's, Newfoundland and Labrador|St. John's]], \"\n",
      " '[[Newfoundland and Labrador|Newfoundland]]\\n'\n",
      " \"| Curling club = [[St. John's Curling Club|St. John's CC]],  <br> [[St. \"\n",
      " \"John's, Newfoundland and Labrador]]\\n\"\n",
      " \"| Skip = '''Brad Gushue'''\\n\"\n",
      " '| Third = [[Mark Nichols (curler)|Mark Nichols]]\\n'\n",
      " '| Second = [[E. J. Harnden]]\\n'\n",
      " '| Lead = [[Geoff Walker (curler)|Geoff Walker]]\\n'\n",
      " '| Alternate = \\n'\n",
      " '| Member Association = {{NL}}\\n'\n",
      " '| Brier appearances = 21 ({{Brier|2003}}, {{Brier|2004}}, {{Brier|2005}}, '\n",
      " '{{Brier|2007}}, {{Brier|2008}}, {{Brier|2009}}, {{Brier|2010}}, '\n",
      " '{{Brier|2011}}, {{Brier|2012}}, {{Brier|2013}}, {{Brier|2014}}, '\n",
      " '{{Brier|2015}}, {{Brier|2016}}, {{Brier|2017}}, {{Brier|2018}}, '\n",
      " '{{Brier|2019}}, {{Brier|2020}}, {{Brier|2021}}, {{Brier|2022}}, '\n",
      " '{{Brier|2023}}, {{Brier|2024}})\\n'\n",
      " '| World Championship appearances = 5 ({{WMCC|2017}}, {{WMCC|2018}}, '\n",
      " '{{WMCC|2022}}, {{WMCC|2023}}, {{WMCC|2024}})\\n'\n",
      " '| World Mixed Doubles Championship appearances = 1 ({{WMDCC|2021}})\\n'\n",
      " '| Pan Continental Championship appearances = 2 ({{PCCC|2022}}, '\n",
      " '{{PCCC|2023}})\\n'\n",
      " '| Olympic appearances = 2 ([[Curling at the 2006 Winter Olympics|2006]], '\n",
      " \"[[Curling at the 2022 Winter Olympics â€“ Men's tournament|2022]])\\n\"\n",
      " '| Top CCA ranking = 1st ([[2016â€“17 curling season|2016â€“17]], [[2017â€“18 '\n",
      " 'curling season|2017â€“18]], [[2021â€“22 curling season|2021â€“22]], [[2022â€“23 '\n",
      " 'curling season|2022â€“23]], [[2023â€“24 curling season|2023â€“24]])\\n'\n",
      " '| Grand Slam victories = 15 ([[2010 The National (January)|2010 National '\n",
      " '(Jan.)]], [[2014 The Masters Grand Slam of Curling|2014 Masters]], [[2014 '\n",
      " 'Canadian Open of Curling|2014 Canadian Open]], [[2015 The National|2015 '\n",
      " \"National]], [[2016 Elite 10]], [[2016 Players' Championship|2016 Players']], \"\n",
      " '[[2017 Meridian Canadian Open|2017 Canadian Open]], [[2017 GSOC Tour '\n",
      " 'Challenge|2017 Tour Challenge]], [[2017 Masters of Curling|2017 Masters]], '\n",
      " \"[[2018 Humpty's Champions Cup|2018 Champions Cup]], [[2018 Elite 10 \"\n",
      " '(September)|2018 Elite 10 (Sept.)]], [[2021 National]], [[2022 Champions Cup '\n",
      " \"(curling)|2022 Champions Cup]], [[2022 National]], [[2024 Players' \"\n",
      " \"Championship|2024 Players']])\\n\"\n",
      " '| medaltemplates =\\n'\n",
      " \"{{MedalSport | Men's [[Curling]] }}\\n\"\n",
      " '{{MedalCountry| {{CAN}} }}\\n'\n",
      " '{{MedalCompetition | [[Winter Olympics]]}}\\n'\n",
      " '{{MedalGold| [[2006 Winter Olympics|2006 Turin]]|[[Curling at the 2006 '\n",
      " \"Winter Olympics#Men's|Team]]}}\\n\"\n",
      " '{{MedalBronze| [[2022 Winter Olympics|2022 Beijing]]|[[Curling at the 2022 '\n",
      " \"Winter Olympics â€“ Men's tournament|Team]]}}\\n\"\n",
      " '{{MedalCompetition | [[World Curling Championships|World Championships]] }}\\n'\n",
      " \"{{MedalGold | [[2017 World Men's Curling Championship|2017 Edmonton]] | }}\\n\"\n",
      " \"{{MedalSilver | [[2018 World Men's Curling Championship|2018 Las Vegas]] | \"\n",
      " '}}\\n'\n",
      " \"{{MedalSilver | [[2022 World Men's Curling Championship|2022 Las Vegas]] | \"\n",
      " '}}\\n'\n",
      " \"{{MedalSilver | [[2023 World Men's Curling Championship|2023 Ottawa]] | }}\\n\"\n",
      " \"{{MedalSilver | [[2024 World Men's Curling Championship|2024 Schaffhausen]] \"\n",
      " '| }}\\n'\n",
      " '{{MedalCompetition | [[World Junior Curling Championships|World Junior '\n",
      " 'Championships]] }}\\n'\n",
      " '{{MedalGold| [[1998 World Junior Curling Championships|1998 Thunder Bay]] | '\n",
      " '}}\\n'\n",
      " '{{MedalGold| [[2001 World Junior Curling Championships|2001 Ogden]] |  }}\\n'\n",
      " '{{MedalCompetition | [[Pan Continental Curling Championships|Pan Continental '\n",
      " 'Championships]]}}\\n'\n",
      " '{{MedalGold|[[2022 Pan Continental Curling Championships|2022 Calgary]]| }}\\n'\n",
      " '{{MedalGold|[[2023 Pan Continental Curling Championships|2023 Kelowna]]| }}\\n'\n",
      " '{{MedalCompetition | [[The Brier]] }}\\n'\n",
      " '{{MedalGold| [[2018 Tim Hortons Brier|2018 Regina]] | }}\\n'\n",
      " '{{MedalGold| [[2023 Tim Hortons Brier|2023 London]] | }}\\n'\n",
      " \"{{MedalGold| [[2024 Montana's Brier|2024 Regina]] | }}\\n\"\n",
      " '{{MedalCountry | {{NL}} }}\\n'\n",
      " '{{MedalCompetition | [[Canadian Olympic Curling Trials|Canadian Olympic '\n",
      " 'Trials]] }}\\n'\n",
      " '{{MedalGold| [[2005 Canadian Olympic Curling Trials|2005 Halifax]] | }}\\n'\n",
      " '{{MedalGold| [[2021 Canadian Olympic Curling Trials|2021 Saskatoon]] | }}\\n'\n",
      " '{{MedalBronze| [[2013 Canadian Olympic Curling Trials|2013 Winnipeg]] | }}\\n'\n",
      " '{{MedalBronze| [[2017 Canadian Olympic Curling Trials|2017 Ottawa]] | }}\\n'\n",
      " '{{MedalCompetition | [[Canadian Mixed Doubles Olympic Curling '\n",
      " 'Trials|Canadian Olympic Mixed Doubles Trials]] }}\\n'\n",
      " '{{MedalSilver | [[2018 Canadian Mixed Doubles Curling Olympic Trials|2018 '\n",
      " 'Portage la Prairie]] | }}\\n'\n",
      " '{{MedalCompetition | [[The Brier]] }}\\n'\n",
      " \"{{MedalGold| [[2017 Tim Hortons Brier|2017 St. John's]] | }}\\n\"\n",
      " '{{MedalGold| [[2020 Tim Hortons Brier|2020 Kingston]] | }}\\n'\n",
      " '{{MedalSilver| [[2007 Tim Hortons Brier|2007 Hamilton]] | }}\\n'\n",
      " '{{MedalSilver| [[2016 Tim Hortons Brier|2016 Ottawa]] | }}\\n'\n",
      " '{{MedalBronze| [[2011 Tim Hortons Brier|2011 London]] | }}\\n'\n",
      " '{{MedalCompetition | [[Canadian Mixed Doubles Curling Championship|Canadian '\n",
      " 'Mixed Doubles Championships]] }}\\n'\n",
      " '{{MedalGold| [[2021 Canadian Mixed Doubles Curling Championship|2021 '\n",
      " 'Calgary]] | }}\\n'\n",
      " '{{MedalCountry | {{flagicon|NL}} Wild Card }}\\n'\n",
      " '{{MedalCompetition | [[The Brier]] }}\\n'\n",
      " '{{MedalGold| [[2022 Tim Hortons Brier|2022 Lethbridge]] | }}\\n'\n",
      " '}}\\n'\n",
      " '\\n'\n",
      " \"'''Bradley Raymond Gushue''', [[Order of Newfoundland and Labrador|ONL]] \"\n",
      " '({{IPAc-en|Ëˆ|g|ÊŠ|Ê’|u}} {{respell|GUU|zhoo}};<ref>{{Cite '\n",
      " 'web|url=https://www.youtube.com/watch?v=ezXmWHOrhDU|title=Sweeps and Stones: '\n",
      " 'A Canadian Curling Story - Episode '\n",
      " '5|website=www.youtube.com|access-date=March 17, 2022}}</ref> born June 16, '\n",
      " \"1980) is a Canadian curler from [[St. John's, Newfoundland and \"\n",
      " 'Labrador]].<ref name=\"Media Guide\">{{Cite '\n",
      " 'web|url=https://www.curling.ca/files/2022/09/2022-PointsBet-Invitational-media-guide-1.pdf|title=2022 '\n",
      " 'Points Bet Invitational Media Guide|website=Curling '\n",
      " 'Canada|access-date=September 17, 2022}}</ref> Gushue, along with teammates '\n",
      " '[[Russ Howard]], [[Mark Nichols (curler)|Mark Nichols]], [[Jamie Korab]] and '\n",
      " '[[Mike Adam]], represented Canada in [[curling]] at the [[2006 Winter '\n",
      " 'Olympics]], where they [[Curling at the 2006 Winter Olympics#Gold medal game '\n",
      " '- Friday, February 24, 1730|won the gold medal]] by defeating Finland 10â€“4. '\n",
      " 'He also represented Canada at the [[Curling at the 2022 Winter Olympics|2022 '\n",
      " 'Winter Olympics]], where he won a bronze medal. In addition to the Olympics, '\n",
      " \"Gushue won the [[2017 World Men's Curling Championship]] with teammates \"\n",
      " '[[Mark Nichols (curler)|Mark Nichols]], [[Brett Gallant]], and [[Geoff '\n",
      " 'Walker (curler)|Geoff Walker]]. He is a record six-time [[Tim Hortons '\n",
      " 'Brier|Brier]] champion skip, having won in [[2017 Tim Hortons Brier|2017]], '\n",
      " '[[2018 Tim Hortons Brier|2018]], [[2020 Tim Hortons Brier|2020]], [[2022 Tim '\n",
      " \"Hortons Brier|2022]], [[2023 Tim Hortons Brier|2023]] and [[2024 Montana's \"\n",
      " 'Brier|2024]] all with Nichols, Gallant and Walker, except for 2023 and 2024 '\n",
      " 'with [[E. J. Harnden]] replacing Gallant. Their win in 2017 was Newfoundland '\n",
      " \"and Labrador's first Brier title in 41 years.  At the [[2018 Tim Hortons \"\n",
      " 'Brier]], Gushue set a new record for Brier game wins as a skip, breaking a '\n",
      " 'three-way tie with previous record-holders [[Russ Howard]] and [[Kevin '\n",
      " 'Martin (curler)|Kevin Martin]].<ref name=\"CurlCan\"/>\\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:54:49.711905Z",
     "start_time": "2024-07-24T16:54:49.707231Z"
    }
   },
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
    "    titles, text = section\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 6 sections, leaving 58 sections.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:55:09.406126Z",
     "start_time": "2024-07-24T16:55:09.398541Z"
    }
   },
   "source": [
    "# print example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    display(ws[1][:77] + \"...\")\n",
    "    print()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brad Gushue']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{Short description|Canadian curler (born 1980)}}\\n{{Infobox curler\\n| name = B...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Brad Gushue', '==Career==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gushue is a six-time ([[1995 Canadian Junior Curling Championships|1995]], [[...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Brad Gushue', '==Career==', '=== Brier appearances ===']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Gushue has played in 21 Briers, Canada's national men's curling championship,...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Brad Gushue', '==Career==', '=== World Championship appearances ===']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'By winning the [[2017 Tim Hortons Brier]], Gushue also earned the right to re...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Brad Gushue', '==Career==', '===2022 Winter Olympics===']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Gushue's team qualified as the Canadian representatives for the [[2022 Winter...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:56:33.185872Z",
     "start_time": "2024-07-24T16:56:33.176624Z"
    }
   },
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:56:39.187268Z",
     "start_time": "2024-07-24T16:56:36.352910Z"
    }
   },
   "source": [
    "# split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 Wikipedia sections split into 73 strings.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T18:25:27.445468Z",
     "start_time": "2024-07-24T18:25:27.441007Z"
    }
   },
   "source": [
    "# print example data\n",
    "pprint(wikipedia_strings[1])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brad Gushue\\n'\n",
      " '\\n'\n",
      " \"'''Bradley Raymond Gushue''', [[Order of Newfoundland and Labrador|ONL]] \"\n",
      " '({{IPAc-en|Ëˆ|g|ÊŠ|Ê’|u}} {{respell|GUU|zhoo}}; born June 16, 1980) is a '\n",
      " \"Canadian curler from [[St. John's, Newfoundland and Labrador]]. Gushue, \"\n",
      " 'along with teammates [[Russ Howard]], [[Mark Nichols (curler)|Mark '\n",
      " 'Nichols]], [[Jamie Korab]] and [[Mike Adam]], represented Canada in '\n",
      " '[[curling]] at the [[2006 Winter Olympics]], where they [[Curling at the '\n",
      " '2006 Winter Olympics#Gold medal game - Friday, February 24, 1730|won the '\n",
      " 'gold medal]] by defeating Finland 10â€“4. He also represented Canada at the '\n",
      " '[[Curling at the 2022 Winter Olympics|2022 Winter Olympics]], where he won a '\n",
      " \"bronze medal. In addition to the Olympics, Gushue won the [[2017 World Men's \"\n",
      " 'Curling Championship]] with teammates [[Mark Nichols (curler)|Mark '\n",
      " 'Nichols]], [[Brett Gallant]], and [[Geoff Walker (curler)|Geoff Walker]]. He '\n",
      " 'is a record six-time [[Tim Hortons Brier|Brier]] champion skip, having won '\n",
      " 'in [[2017 Tim Hortons Brier|2017]], [[2018 Tim Hortons Brier|2018]], [[2020 '\n",
      " 'Tim Hortons Brier|2020]], [[2022 Tim Hortons Brier|2022]], [[2023 Tim '\n",
      " \"Hortons Brier|2023]] and [[2024 Montana's Brier|2024]] all with Nichols, \"\n",
      " 'Gallant and Walker, except for 2023 and 2024 with [[E. J. Harnden]] '\n",
      " \"replacing Gallant. Their win in 2017 was Newfoundland and Labrador's first \"\n",
      " 'Brier title in 41 years.  At the [[2018 Tim Hortons Brier]], Gushue set a '\n",
      " 'new record for Brier game wins as a skip, breaking a three-way tie with '\n",
      " 'previous record-holders [[Russ Howard]] and [[Kevin Martin (curler)|Kevin '\n",
      " 'Martin]].<ref name=\"CurlCan\"/>')\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:58:25.000141Z",
     "start_time": "2024-07-24T16:58:22.049267Z"
    }
   },
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response.data):\n",
    "        assert i == be.index  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e.embedding for e in response.data]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:58:46.853077Z",
     "start_time": "2024-07-24T16:58:46.846017Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  \\\n",
       "0  Brad Gushue\\n\\n{{Short description|Canadian cu...   \n",
       "1  Brad Gushue\\n\\n'''Bradley Raymond Gushue''', [...   \n",
       "2  Brad Gushue\\n\\n==Career==\\n\\nGushue is a six-t...   \n",
       "3  Brad Gushue\\n\\n==Career==\\n\\n=== Brier appeara...   \n",
       "4  Brad Gushue\\n\\n==Career==\\n\\n=== Brier appeara...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.024818697944283485, -0.01212653424590826, ...  \n",
       "1  [-0.04559198394417763, 0.018672654405236244, 0...  \n",
       "2  [-0.024634655565023422, 0.022033650428056717, ...  \n",
       "3  [-0.021867547184228897, 0.00506884278729558, 0...  \n",
       "4  [-0.02394963800907135, 0.013273593038320541, 0...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brad Gushue\\n\\n{{Short description|Canadian cu...</td>\n",
       "      <td>[-0.024818697944283485, -0.01212653424590826, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brad Gushue\\n\\n'''Bradley Raymond Gushue''', [...</td>\n",
       "      <td>[-0.04559198394417763, 0.018672654405236244, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brad Gushue\\n\\n==Career==\\n\\nGushue is a six-t...</td>\n",
       "      <td>[-0.024634655565023422, 0.022033650428056717, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brad Gushue\\n\\n==Career==\\n\\n=== Brier appeara...</td>\n",
       "      <td>[-0.021867547184228897, 0.00506884278729558, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brad Gushue\\n\\n==Career==\\n\\n=== Brier appeara...</td>\n",
       "      <td>[-0.02394963800907135, 0.013273593038320541, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T17:00:59.766577Z",
     "start_time": "2024-07-24T17:00:59.702329Z"
    }
   },
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "SAVE_PATH = current_dir + \"/data/winter_olympics_2022.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
